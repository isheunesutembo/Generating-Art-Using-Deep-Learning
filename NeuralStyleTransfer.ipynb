{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Art Geneneration Using Deep Learning (Neural Style Transfer)\n",
    "\n",
    "<p>Deep Learning has been the most important important branch of machine learning and particularly Computer Vision which has led to the rise of self-driving cars and facial recognition. One of the most  fascinating application of Computer Vision using Convolutional Neural Networks is the generation of art images.The algorithm uses neural representations to separate and recombine content and style images ,providing a neural algorithm for the creation of artistic images.To understand how convolutional neural networks are able to generate art images, we need to understand how layers in convolutional neural networks work.</p>\n",
    "\n",
    "## What are deep convolutional neural networks learning ?\n",
    "<p>Inorder to understand what are convolutional neural networks learning we need to pick a unit in each layer and find 9 image patches that maximize the unit's activation hidden unit.</p>\n",
    "\n",
    "![Layer 1](./images/layer1.png)\n",
    "\n",
    "<p>The above image shows hidden units in layer 1 and if we take a look at the first 9 image patches in the top left corner in layer 1 we can see that the image patches are activated by a diagonal line in the image. We can also see that the 9 image patches below those first 9 image patches in the top left corner are activated by an orange color and that's what activates the hidden units.</p>\n",
    "\n",
    "![Layer2](images/layer2.png)\n",
    "\n",
    "<p>We can see that layer 2 is now detecting compex shapes and patterns we can also see that 9 image patches below those in the top left corner are also activated by an orange color.</p>\n",
    "\n",
    "![Layer 3](images/layer3.png)\n",
    "\n",
    "<p>In layer 3 things start to get a little fascinating we can see that the 9 image patches in the top left corner hidden unit are activated by sort of a honey comb structure.We can also notice that they is a hidden unit that is activated by sort of circle or car wheel structures and they is also a hidden unit that is activated by people. As we go deeper into the convnet the layers of the convnet will start to see larger parts of the image.</p>\n",
    "\n",
    "![layer 4](images/layer4.png)\n",
    "\n",
    "<p>In layer 4 we can see that the first 9 image patches in the top left corner are activated by dogs .They are also other hideen units activated by water,birds and animal legs.</p>\n",
    "\n",
    "<p>Now that we have gained an intuition about what shallow and deepr layers of a convolutional neural network are learning let's now use this knowledge to understanf how neural style transfer works. If you want to gain more knowledge about how convnets learn you may read this research paper in the link below.</p>\n",
    "\n",
    "\n",
    "[Understanding and visualizing convolutional neural networks paper](https://arxiv.org/abs/1311.2901)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "<p>Our problem formulation in neural style transfer is how do we generate a new art image given a content image p which we wish to keep and a style image a which we also want to keep </p>\n",
    "\n",
    "<p>Content Image</p>\n",
    "\n",
    "![Content Image](images/contentimage.png)\n",
    "\n",
    "<p>Style Image</p>  \n",
    "\n",
    "![Style Image](images/styleimage.png)\n",
    "<p>Inorder to generate a new art image x , which is blended from the content image and the style image a ,we need to make sure that  x  returns the content features such as eyes ,ears and head from the content image p and also preserving the style from style image a.Thus we initialize our image x to be generated randomly into a white color image and we then use gradient decent the cost function of the generated image which we denote as J(G). J(G) the cost function of the generated image is the total cost function of the content image J(C) and that of the style image J(S)</p>\n",
    "\n",
    "<p>The purpose of both the content loss and the style loss is to make sure that the generated image returns both features from the content image and the style image.The full explanation of the content loss and the style loss is below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for content image\n",
    "cImPath=r'C:\\Users\\isheunesu\\neuralstyle\\DSC_0061_3.JPG'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for the style image\n",
    "sImPath=r'C:\\Users\\isheunesu\\neuralstyle\\pic.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for the generated image\n",
    "genImOutputPath=r\"C:\\Users\\isheunesu\\neuralstyle\\generated.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetHeight = 512\n",
    "targetWidth = 512\n",
    "targetSize = (targetHeight, targetWidth)\n",
    "\n",
    "cImageOrig = Image.open(cImPath)\n",
    "cImageSizeOrig = cImageOrig.size\n",
    "cImage = load_img(path=cImPath, target_size=targetSize)\n",
    "cImArr = img_to_array(cImage)\n",
    "cImArr = K.variable(preprocess_input(np.expand_dims(cImArr, axis=0)), dtype='float32')\n",
    "\n",
    "sImage = load_img(path=sImPath, target_size=targetSize)\n",
    "sImArr = img_to_array(sImage)\n",
    "sImArr = K.variable(preprocess_input(np.expand_dims(sImArr, axis=0)), dtype='float32')\n",
    "\n",
    "gIm0 = np.random.randint(256, size=(targetWidth, targetHeight, 3)).astype('float64')\n",
    "gIm0 = preprocess_input(np.expand_dims(gIm0, axis=0))\n",
    "\n",
    "gImPlaceholder = K.placeholder(shape=(1, targetWidth, targetHeight, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_reps(x, layer_names, model):\n",
    "    featMatrices = []\n",
    "    for ln in layer_names:\n",
    "        selectedLayer = model.get_layer(ln)\n",
    "        featRaw = selectedLayer.output\n",
    "        featRawShape = K.shape(featRaw).eval(session=tf_session)\n",
    "        N_l = featRawShape[-1]\n",
    "        M_l = featRawShape[1]*featRawShape[2]\n",
    "        featMatrix = K.reshape(featRaw, (M_l, N_l))\n",
    "        featMatrix = K.transpose(featMatrix)\n",
    "        featMatrices.append(featMatrix)\n",
    "    return featMatrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Loss\n",
    "<p>The purpose of the content loss is to make sure that our generated image x retains some \"global\" charecteristics of the content image p.For example in our case we want to make sure that the generated image x looks like the house image in p. This means that shapes such windows and roof top ought to be recognizable.To archive this the content loss is defined as the mean squared error between the feature represantation p and x , respectively at a given layer l.For content loss layer l is neither too deep or too shallow.</p>\n",
    "\n",
    "![Content Loss](images/contenttloss.png)\n",
    "\n",
    "<p>The derivative of this loss with respect to the activations in layer l equals</p>\n",
    "\n",
    "![Derivative Of The Content Loss](images/contentderivative.png)\n",
    "<p>Remeber the image x we initialized randomly we can change that unitl it generates the same response in a certain layer of the CNN as the original image p by computing the gradient descent of the loss function.</p>\n",
    "\n",
    "<ul>\n",
    "    <li>F and P are matrices with a number of rows equal to N and a number of columns equal to M</li>\n",
    "    <li>N is the number of filters in layer l and M is the number of spatial elements in the feature map (height times width) for layer l</li>\n",
    "    <li>F contains the feature representation of x for layer l</li>\n",
    "    <li>P contains the feature representation of p for layer l</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(F, P):\n",
    "    cLoss = 0.5*K.sum(K.square(F - P))\n",
    "    return cLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gram_matrix(F):\n",
    "    G = K.dot(F, K.transpose(F))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Loss\n",
    "<p>The style loss is designed to preserve stylistic charecteristics of the style image, a .Rather than using the difference between the feature representations , the authors use the difference betweenthe Gram matrics from selected layers , where the Gram matrix is defined as </p>\n",
    "\n",
    "![Gram Matrix](images/grammatrix.png)\n",
    "\n",
    "<p>The Gram matrix is a square matrix that contains the dot products between each vectorized filter in layer l. The Gram matrix can therefore be thought of as a non-normalized correlation matrix for filters in layer l.Where A is the gram matrix of the style image a and G is the gram matrix of the generated image x</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Total loss ](images/styleloss.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p>Ascending layers in most Convolutional Neural Networks such as VGG have increasingly larger receptive fields . As this receptive field grows , more large scale charecteristics of the input image are preserved .Because of this multiple layers should be selected for the style to incorperate global stylistic qualities .To create a blending between these layers , we assign a weight w and define the total loss as</p>\n",
    "\n",
    "![](images/stylelosstotal.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(ws,Gs,As):\n",
    "    sloss=K.variable(0.)\n",
    "    for w,G,A in zip(ws,Gs,As):\n",
    "        M_1=K.int_shape(G)[1]\n",
    "        N_1=K.int_shape(G)[0]\n",
    "        G_gram=get_Gram_matrix(G)\n",
    "        A_gram=get_Gram_matrix(A)\n",
    "        sloss+=w*0.25*K.sum(K.square(G_gram-A_gram))//(N_1**2*M_1**2)\n",
    "        return sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss\n",
    "<p>To generate images that mix content of a photograph with the styling of a painting we jointly minimize the distance of a white noise image that we initialised randomly from the content representation of the photograph in one layer of the network and the style representation of the painting in a number of layers of the CNN.Let p be the photograph and a be the artwork.The loss we minimize is</p>\n",
    "\n",
    "![](images/totalloss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_loss(gImPlaceholder, alpha=1.0, beta=10000.0):\n",
    "    F = get_feature_reps(gImPlaceholder, layer_names=[cLayerName], model=gModel)[0]\n",
    "    Gs = get_feature_reps(gImPlaceholder, layer_names=sLayerNames, model=gModel)\n",
    "    contentLoss = get_content_loss(F, P)\n",
    "    styleLoss = get_style_loss(ws, Gs, As)\n",
    "    totalLoss = alpha*contentLoss + beta*styleLoss\n",
    "    return totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(gImArr):\n",
    "    \"\"\"\n",
    "    Calculate total loss using K.function\n",
    "    \"\"\"\n",
    "    if gImArr.shape != (1, targetWidth, targetWidth, 3):\n",
    "        gImArr = gImArr.reshape((1, targetWidth, targetHeight, 3))\n",
    "    loss_fcn = K.function([gModel.input], [get_total_loss(gModel.input)])\n",
    "    return loss_fcn([gImArr])[0].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(gImArr):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the loss function with respect to the generated image\n",
    "    \"\"\"\n",
    "    if gImArr.shape != (1, targetWidth, targetHeight, 3):\n",
    "        gImArr = gImArr.reshape((1, targetWidth, targetHeight, 3))\n",
    "    grad_fcn = K.function([gModel.input], K.gradients(get_total_loss(gModel.input), [gModel.input]))\n",
    "    grad = grad_fcn([gImArr])[0].flatten().astype('float64')\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_array(x):\n",
    "    # Zero-center by mean pixel\n",
    "    if x.shape != (targetWidth, targetHeight, 3):\n",
    "        x = x.reshape((targetWidth, targetHeight, 3))\n",
    "    x[..., 0] += 103.939\n",
    "    x[..., 1] += 116.779\n",
    "    x[..., 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[..., ::-1]\n",
    "    x = np.clip(x, 0, 255)\n",
    "    x = x.astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess_array(x):\n",
    "    x = np.expand_dims(x.astype('float64'), axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save generated image\n",
    "def save_original_size(x,target_size=cImageSizeOrig):\n",
    "    xIm=Image.fromarray(x)\n",
    "    xIm=xIm.resize(target_size)\n",
    "    xIm.save(genImOutputPath)\n",
    "    return xIm\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Art\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "tf_session = K.get_session()\n",
    "print('Generating Art')\n",
    "cModel = VGG16(include_top=False, weights='imagenet', input_tensor=cImArr)\n",
    "sModel = VGG16(include_top=False, weights='imagenet', input_tensor=sImArr)\n",
    "gModel = VGG16(include_top=False, weights='imagenet', input_tensor=gImPlaceholder)\n",
    "cLayerName = 'block4_conv2'#Content Layer\n",
    "sLayerNames = [\n",
    "                'block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1'\n",
    "                #'block5_conv1'\n",
    "                ]#Style Layers\n",
    "\n",
    "P = get_feature_reps(x=cImArr, layer_names=[cLayerName], model=cModel)[0]\n",
    "As = get_feature_reps(x=sImArr, layer_names=sLayerNames, model=sModel)\n",
    "ws = np.ones(len(sLayerNames))/float(len(sLayerNames))\n",
    "\n",
    "iterations = 600\n",
    "x_val = gIm0.flatten()\n",
    "start = time.time()\n",
    "xopt, f_val, info= fmin_l_bfgs_b(calculate_loss, x_val, fprime=get_grad,\n",
    "                            maxiter=iterations, disp=True)\n",
    "\n",
    "xOut = postprocess_array(xopt)\n",
    "xIm = save_original_size(xOut)\n",
    "print( 'Image saved')\n",
    "end = time.time()\n",
    "\n",
    "print ('Time taken: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://medium.com/mlreview/making-ai-art-with-style-transfer-using-keras-8bb5fa44b216\n",
    "\n",
    "\n",
    "[A Neural Algorithm of Artistic Style Paper](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "https://github.com/llSourcell/AI_Artist\n",
    "\n",
    "[Contact Me On Twitter](https://twitter.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
